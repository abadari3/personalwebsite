<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Automatic Speed Detection from Dashcam Video</title>
    <style>
        /* Style the button that is used to open and close the collapsible content */
        .collapsible {
          background-color: transparent;
          color: white;
          cursor: pointer;
          width: auto;
          border: none;
          text-align: left;
          outline: none;
          font-size: 15px;
          text-decoration: none;
        }
  
        /* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
        .collapsible-active, .collapsible:hover {
          text-decoration: none;
        }
  
        /* Style the collapsible content. Note: hidden by default */
        .collapsible-content {
          max-height: 100%;
          padding: 0px;
          overflow: hidden;
          transition: max-height 0.2s ease-out;
        }
  
        /* Style the collapsible content. Note: shown by default */
        .collapsible-content-shown-by-default {
          max-height: 100%;
          padding: 0px;
          overflow: hidden;
          transition: max-height 0.2s ease-out;
        }
  
        .disabled{
          pointer-events:none;
          /* font-style:italic; */
          color:darkgray
        }
  
        .home{
          color:black
        }
  
        body {
          background-color: #fff;
          padding:50px;
          font: 14px/1.5 "Noto Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
          /* color:#727272; */
          /* color:darkslategray; */
          color:black;
          font-weight:400;
        }
  
        h1, h2, h3, h4, h5, h6 {
          /* color:#222; */
          color:black;
          margin:0 0 20px;
        }
  
        p, ul, ol, table, pre, dl {
          margin:0 0 20px;
        }
  
        h1, h2, h3 {
          line-height:1.1;
        }
  
        h1 {
          font-size:28px;
        }
  
        h2 {
          /* color:#393939; */
          color:black;
        }
  
        h3, h4, h5, h6 {
          /* color:#494949; */
          color:black;
        }
  
        a {
          /* color:#39c; */
          color:dodgerblue;
          text-decoration:none;
        }
  
        a:hover {
          /* color:#069; */
          color:darkblue;
        }
  
        a small {
          font-size:11px;
          color:#777;
          margin-top:-0.3em;
          display:block;
        }
  
        a:hover small {
          color:#777;
        }
  
        .wrapper {
          width:860px;
          margin:0 auto;
        }
  
        blockquote {
          border-left:1px solid #e5e5e5;
          margin:0;
          padding:0 0 0 20px;
          font-style:italic;
        }
  
        code, pre {
          font-family:Monaco, Bitstream Vera Sans Mono, Lucida Console, Terminal, Consolas, Liberation Mono, DejaVu Sans Mono, Courier New, monospace;
          color:#333;
          font-size:12px;
        }
  
        pre {
          padding:8px 15px;
          background: #f8f8f8;
          border-radius:5px;
          border:1px solid #e5e5e5;
          overflow-x: auto;
        }
  
        table {
          width:100%;
          border-collapse:collapse;
        }
  
        th, td {
          text-align:left;
          padding:5px 10px;
          border-bottom:1px solid #e5e5e5;
        }
  
        dt {
          color:#444;
          /* color:black; */
          font-weight:700;
        }
  
        th {
          color:#444;
        }
  
        img {
          max-width:100%;
        }
  
        header {
          width:270px;
          float:left;
          position:fixed;
          -webkit-font-smoothing:subpixel-antialiased;
        }
  
        header ul {
          list-style:none;
          height:40px;
          padding:0;
          background: #f4f4f4;
          border-radius:5px;
          border:1px solid #e0e0e0;
          width:270px;
        }
  
        header li {
          width:89px;
          float:left;
          border-right:1px solid #e0e0e0;
          height:40px;
        }
  
        header li:first-child a {
          border-radius:5px 0 0 5px;
        }
  
        header li:last-child a {
          border-radius:0 5px 5px 0;
        }
  
        header ul a {
          line-height:1;
          font-size:11px;
          color:#999;
          display:block;
          text-align:center;
          padding-top:6px;
          height:34px;
        }
  
        header ul a:hover {
          color:#999;
        }
  
        header ul a:active {
          background-color:#f0f0f0;
        }
  
        strong {
          /* color:#222; */
          color:black;
          font-weight:700;
        }
  
        header ul li + li + li {
          border-right:none;
          width:89px;
        }
  
        header ul a strong {
          font-size:14px;
          display:block;
          /* color:#222; */
          color:black;
        }
  
        section {
          width:500px;
          float:right;
          padding-bottom:50px;
        }
  
        small {
          font-size:11px;
        }
  
        hr {
          border:0;
          background:#e5e5e5;
          height:1px;
          margin:0 0 20px;
        }
  
        footer {
          width:270px;
          float:left;
          position:fixed;
          bottom:50px;
          -webkit-font-smoothing:subpixel-antialiased;
        }
  
        figure {
          margin: auto;
        }
  
        figcaption {
          background-color: white;
          color: black;
          font-style: italic;
          text-align: center;
        }
  
        @media print, screen and (max-width: 960px) {
  
          div.wrapper {
            width:auto;
            margin:0;
          }
  
          header, section, footer {
            float:none;
            position:static;
            width:auto;
          }
  
          header {
            padding-right:320px;
          }
  
          section {
            border:1px solid #e5e5e5;
            border-width:1px 0;
            padding:20px 0;
            margin:0 0 20px;
          }
  
          header a small {
            display:inline;
          }
  
          header ul {
            position:absolute;
            right:50px;
            top:52px;
          }
        }
  
        @media print, screen and (max-width: 720px) {
          body {
            word-wrap:break-word;
          }
  
          header {
            padding:0;
          }
  
          header ul, header p.view {
            position:static;
          }
  
          pre, code {
            word-wrap:normal;
          }
        }
  
        @media print, screen and (max-width: 480px) {
          body {
            padding:15px;
          }
  
          header ul {
            width:99%;
          }
  
          header li, header ul li + li + li {
            width:33%;
          }
        }
  
        @media print {
          body {
            padding:0.4in;
            font-size:12pt;
            /* color:#444; */
            color:black;
          }
        }
  
        /*!
        * GitHub Light v0.4.1
        * Copyright (c) 2012 - 2017 GitHub, Inc.
        * Licensed under MIT (https://github.com/primer/github-syntax-theme-generator/blob/master/LICENSE)
        */
  
        .pl-c /* comment, punctuation.definition.comment, string.comment */ {
          color: #6a737d;
        }
  
        .pl-c1 /* constant, entity.name.constant, variable.other.constant, variable.language, support, meta.property-name, support.constant, support.variable, meta.module-reference, markup.raw, meta.diff.header, meta.output */,
        .pl-s .pl-v /* string variable */ {
          color: #005cc5;
        }
  
        .pl-e /* entity */,
        .pl-en /* entity.name */ {
          color: #6f42c1;
        }
  
        .pl-smi /* variable.parameter.function, storage.modifier.package, storage.modifier.import, storage.type.java, variable.other */,
        .pl-s .pl-s1 /* string source */ {
          color: #24292e;
        }
  
        .pl-ent /* entity.name.tag, markup.quote */ {
          color: #22863a;
        }
  
        .pl-k /* keyword, storage, storage.type */ {
          color: #d73a49;
        }
  
        .pl-s /* string */,
        .pl-pds /* punctuation.definition.string, source.regexp, string.regexp.character-class */,
        .pl-s .pl-pse .pl-s1 /* string punctuation.section.embedded source */,
        .pl-sr /* string.regexp */,
        .pl-sr .pl-cce /* string.regexp constant.character.escape */,
        .pl-sr .pl-sre /* string.regexp source.ruby.embedded */,
        .pl-sr .pl-sra /* string.regexp string.regexp.arbitrary-repitition */ {
          color: #032f62;
        }
  
        .pl-v /* variable */,
        .pl-smw /* sublimelinter.mark.warning */ {
          color: #e36209;
        }
  
        .pl-bu /* invalid.broken, invalid.deprecated, invalid.unimplemented, message.error, brackethighlighter.unmatched, sublimelinter.mark.error */ {
          color: #b31d28;
        }
  
        .pl-ii /* invalid.illegal */ {
          color: #fafbfc;
          background-color: #b31d28;
        }
  
        .pl-c2 /* carriage-return */ {
          color: #fafbfc;
          background-color: #d73a49;
        }
  
        .pl-c2::before /* carriage-return */ {
          content: "^M";
        }
  
        .pl-sr .pl-cce /* string.regexp constant.character.escape */ {
          font-weight: bold;
          color: #22863a;
        }
  
        .pl-ml /* markup.list */ {
          color: #735c0f;
        }
  
        .pl-mh /* markup.heading */,
        .pl-mh .pl-en /* markup.heading entity.name */,
        .pl-ms /* meta.separator */ {
          font-weight: bold;
          color: #005cc5;
        }
  
        .pl-mi /* markup.italic */ {
          font-style: italic;
          color: #24292e;
        }
  
        .pl-mb /* markup.bold */ {
          font-weight: bold;
          color: #24292e;
        }
  
        .pl-md /* markup.deleted, meta.diff.header.from-file, punctuation.definition.deleted */ {
          color: #b31d28;
    background-color: #ffeef0;
  }
  
  .pl-mi1 /* markup.inserted, meta.diff.header.to-file, punctuation.definition.inserted */ {
    color: #22863a;
    background-color: #f0fff4;
  }
  
  .pl-mc /* markup.changed, punctuation.definition.changed */ {
    color: #e36209;
    background-color: #ffebda;
  }
  
  .pl-mi2 /* markup.ignored, markup.untracked */ {
    color: #f6f8fa;
    background-color: #005cc5;
  }
  
  .pl-mdr /* meta.diff.range */ {
    font-weight: bold;
    color: #6f42c1;
  }
  
  .pl-ba /* brackethighlighter.tag, brackethighlighter.curly, brackethighlighter.round, brackethighlighter.square, brackethighlighter.angle, brackethighlighter.quote */ {
    color: #586069;
  }
  
  .pl-sg /* sublimelinter.gutter-mark */ {
    color: #959da5;
  }
  
  .pl-corl /* constant.other.reference.link, string.other.link */ {
    text-decoration: underline;
    color: #032f62;
  }
  
  .center {
    display: block;
    margin-left: auto;
    margin-right: auto;
    /* width: 50%; */
  }
      </style>
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a class="home" href="dashcam">Automatic Speed Detection from Dashcam Video</a></h1>
        <hr>
        <h3>CS 4476: Computer Vision</h3>
        <p>Ananda Badari<br>
        Evan Juncal<br>
        Kevin Park<br>
        Rohit Ramakrishnan</p>
        <hr>
        <div align='center'>
          <a href="dashcamproposal">Project Proposal</a><br>
          <a href="dashcammidterm">Midterm Update</a><br>
          <a href="dashcamfinal">Final Update</a>
        </div>
        <br>
        <hr>
        <img src="https://www.roboticsbusinessreview.com/wp-content/uploads/2019/08/AdobeStock_222100885.jpeg">

        <img src="https://i.pinimg.com/originals/ed/d3/3f/edd33f73975cbf964c863990f514247d.jpg">
        
        <img src="https://geospatialmedia.s3.amazonaws.com/wp-content/uploads/2017/08/LiDAR_detection-matrix_%C2%A9LeddarTech.jpg">

        <p class="view"><a href="https://github.gatech.edu/abadari3/Automatic-Speed-Detection-from-Dashcam-Video">View the Project on GitHub <small>Automatic Speed Detection from Dashcam Video</small></a></p>


      </header>
      <section>
        <h1 align='center'>Final Update</h1>

        <button class="collapsible" id="collapsible-button-0" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><h3>Abstract</h3></button>
        <div class="collapsible-content" id="collapsible-section-0">
          <p>
            We aim to train and test a supervised learning model that estimates the real-world speed of a vehicle given its dashcam video. This is an attempt at solving the <a target="_blank" href="https://github.com/commaai/speedchallenge">comma.ai programming challenge</a>. Our general approach is to create an Optical Flow map and then use a variation of the NVIDIA CNN to estimate speeds based on the optical flows. We are using SIFT to create a lane-detector for each frame in the data set, to construct a sparse optical flow map by comparing positions of the lanes. Additionally, we are also using the Farneback method to estimate the direction and speed of each pixel between images by creating a dense optical flow map. In the <a target="_blank" href="dashcammidterm">midterm update</a>, we developed the preprocessing and optical-flow portions of our model by implementing a SIFT-based lane-detector and a Farneback Dense Optical Flow Map. In this final update, we finalized our CNN model, tweaked the image pre-processing, and improved our MSE (mean squared error) against the actual output.
          </p>

          <figure>
            <!-- <a target="_blank" href="https://www.youtube.com/watch?v=e-kVuEyt19A">
              <img src="media/output.gif" style="width:100%">
            </a> -->
            <iframe class="center" width="420" height="345" src="https://www.youtube.com/embed/e-kVuEyt19A">
            </iframe>
            <figcaption>True Speeds over Training Data</figcaption>
          </figure><br>
          <hr>
        </div>
        

        <button class="collapsible" id="collapsible-button-1" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><h3>Introduction</h3></button>
        <div class="collapsible-content" id="collapsible-section-1">
          <p>
            Finding the speed from a dashcam camera has important applications in autonomous vehicles, from self-driving cars to drones. For our specific problem, given a dashcam video of a car, we want to assign each frame with the approximate instantaneous speed during that frame. Our input domain are dashcam videos from a car or other road-based vehicle, specifically 20fps 640x480 video in this case, and our expected output is the instantaneous speed for each frame of the video. 

            <br><br>

            We are using two different ways of computing an optical map, using SIFT for a sparse optical map and using Farneback method for a dense optical map. Then, we can run this information from the optical maps through the NVIDIA self-driving CNN to estimate speeds given any dashcam video. We then compare the performance between the two approaches.

            <br><br>
            A lot of previous implementations / prior work calculated dense optical flows on preprocessed frames and fed that into a CNN. <br>
            Our approach with the Farneback method for optical flows is similar to contemporary methods except we are focusing and trying out the pre-processing methods we learned in class. Also, we used NVIDIA CNN model, which is parametrized and constructed differently from other CNN structures.
            <br><br>
            In our other approach, calculating sparse optical flows via lane detection calculated by SIFT hasn’t been attempted yet, from what we have seen. We used the MLP regressor model via the sklearn library.
            </ul> 
          </p>
          <hr>
        </div>

        <button class="collapsible" id="collapsible-button-2" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><h3>Approach</h3></button>
        <div class="collapsible-content" id="collapsible-section-2">
          
          <p>
            <button class="collapsible" id="collapsible-button-20" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><strong>SIFT, Lane Detection, Homography</strong></button>
            <div class="collapsible-content" id="collapsible-section-20">            
              The approach of this method is to use SIFT to retrieve and match keypoints between each frame, then use homography calculation to figure out the scaling (“zoom in”) factor, which can be correlated with the speed of the vehicle.<br><br>
              However, there was one major issue where many of the keypoints returned by SIFT are pinned on other vehicles on the road. This disrupts accurate homography calculation because other vehicles are traveling at a similar speed which makes them look “stationary”. To resolve this issue, we implemented lane detection to filter out the SIFT keypoints to road markings.
              <br><br>
            </div>

            <button class="collapsible" id="collapsible-button-21" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><strong>Dense Optical Map</strong></button>
            <div class="collapsible-content" id="collapsible-section-21">            
              <br>
              The first thing that we did is to convert the training dataset (train.mp4) into individual frames, and save them as images for easier access and manipulation. Then, for preprocessing the images, we grayscaled and cropped out the top and bottom of the images, to get rid of the dashboard or any static car-items from the frame, and got rid of the sky to remove irrelevant data that will confuse the CNN. Then, we ran a canny edge detector on the preprocessed images to isolate edges to help match up different consecutive frames. Then, we can utilize the Farneback Dense Optical Flow algorithm on the images, with the canny edges detected, to construct a dense optical flow map.
              <br>
            </div>

            <button class="collapsible" id="collapsible-button-22" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><strong><br>CNN</strong><br></button>
            <div class="collapsible-content" id="collapsible-section-22">            
              <br>To build the CNN we utilized the architecture outlined in a paper written by NVIDIA. In the paper, they used the CNN to map input from a front facing dash camera to steering commands <a href="#collapsible-section-6">[1]</a>. We used a slightly adapted architecture to predict the speed of the car instead.

              <figure align="center">
                <img class="center" src="media/nvidia.png" style="width:60%">
                <figcaption>CNN Architecture designed by NVIDIA.</figcaption>
              </figure><br>

              Before passing frame data into the CNN, we cropped the frames and then resized them to the input size expected by the CNN. Each frame was cropped to remove a portion of the sky and the hood of the car which can be seen in each frame. Once the frames were cropped, they were resized to (66x200x3).
            </div>

            
            <button class="collapsible" id="collapsible-button-23" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><strong><br>Obstacles</strong><br></button>
            <div class="collapsible-content" id="collapsible-section-23">            
              <br>Some of our obstacles were:<br><br>
              <ul>
                <li><b>Memory overflow error.</b> Since we are working with 20400, 480 x 640 frames (when B&W), simply appending that to a list / NumPy array during training wasn’t feasible as the RAM would easily be used up. We had to use techniques like batching that would minimize the number of local variables being stored, and the size of the local variables.</li><br>
                <li><b>Time.</b> The time it took to train the data was considerable, so we had to program more efficient code and look into multithreading. For the CNN implementation, we found running the script via the GPU (specifically NVIDIA GPU with CUDA enabled) compared to the CPU, sped up the time it took from ~2 hours to  ~10 minutes per run. This allowed us to do more testing easily, and refine the MSE the best we could.
                </li><br>
                <li><b>High MSE.</b> For both approaches, our MSE was initially ranging near ~70 to ~90. For the dense optical flow approach, we had to look back at the preprocessing phase and make some changes. We decided to increase the threshold value of the Canny edge detector so that really only the edges of the lane markings were shown, and cropped the image differently. This landed us an MSE around ~40 to ~50.</li>
            </div>


          </p>
          <hr>
        </div>

        <button class="collapsible" id="collapsible-button-3" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><h3>Experiments and Results</h3></button>
        <div class="collapsible-content" id="collapsible-section-3">
          <p>
            For our experimental setup, we had access to a training dataset (train.mp4) that comprised 20400 individual frames, and labelled speed data for every frame. Below are the true speeds of the car based on the labelled frames.<br><br>

            <figure align="center">
              <img src="media/truespeed.png" style="width:100%">
              <figcaption>True Speed at a Specific Frame.</figcaption>
            </figure><br>

            From these 20400 labelled frames, we split the data 75% into a training set and 25% into a testing set for a CNN implementation, and a 80%-20% train/test split for the Sift-based speed detection. <br><br>

            In terms of an evaluation metric, we will be using MSE (Mean Squared Error) on the test data between the predicted speeds and the true labelled speeds. As described in the comma.ai challenge, an MSE value of below 10 is good, below 5 is great, and below 3 is amazing, and this will be the baseline to evaluate the effectiveness of our approach. <br><br>

            <button class="collapsible" id="collapsible-button-30" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><strong>SIFT, Lane Detection, Homography</strong></button>
            <div class="collapsible-content" id="collapsible-section-30">
              The lane detection algorithm utilizes two major techniques learned in class: canny edge and Hough transformation. Canny edge detection is used to extract white lane markings on the road in contrast to the black asphalt. Then, Hough line transformation locates where straight lines are located in pixel values.
              <br><br>Within the lane detection, the sensitivity of canny edge and Hough line transformation can be tweaked. Through our experimentation, the canny edge min and max sensitivity values are currently set as 50 and 130, respectively. And the line detected by Hough transformation must be at least 20 pixels long.
              <br><br>There are occasions when lane detection can fail. For example, there aren’t any lane markings in the direction of the vehicle’s travel at the intersection. In such case, the algorithm will fall back to use SIFT keypoints in the general region where the road is located. If all else fails, then it will simply use every SIFT keypoints it can find.
              <br><br>
              Our MSE from this approach consistently averaged between 57 and 60. This result is relatively high. When there are lots of SIFT keypoints detected and paired between frames, homography can be more easily determined. However, in dark frames, proper homography can't be determined because SIFT cannot detect road markings reliably. <br><br>
              <figure align="center">
                <img src="media/siftsuccess.png" style="width:100%">
                <figcaption>Successful Case</figcaption>
              </figure><br>
              <figure align="center">
                <img src="media/siftfailure.png" style="width:100%">
                <figcaption>Failure Case</figcaption>
              </figure><br>

              <br><br>
            </div>

            <button class="collapsible" id="collapsible-button-31" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><strong>Dense Optical Map</strong></button>
            <div class="collapsible-content" id="collapsible-section-31">            
              <br>For the image preprocessing as described in approach, the parameters that we could change are the amount by which we crop the images, and how much our sigma value is for the canny edge detector. Resizing the images, equalizing brightness, and many other things can be done here. As for the canny edge detector, while raising the sigma value might help not detect smaller edges, it may miss lanes completely, making optical flow more difficult. <br><br>

              As for the actual optical flow algorithm, there are many parameters to the Gunnar Farneback algorithm. pyr_scale specifies the image scale to build pyramids for each image. In our case we used a 0.5 pyr_scale value which represents a classical pyramid, where each layer is half the size of the next layer. This didn’t quite make a difference to us, as we set levels=1, which means no additional layers are constructed. win_size is the averaging window size, where larger values improve the model’s performance to noise and fast motion, but result in a more blurred map. We had the algorithm perform iterations=2 at our layer, increasing the number of iterations may improve performance, but also decreases the speed significantly. For the poly_n, poly_sigma parameters, these are the size of the pixel neighborhood and the standard deviation of the gaussian to smooth over. For these, we used the typical values of poly_n = 5, poly_sigma = 1.3. Larger values for poly_n and poly_sigma would mean more smoothing and better approximated for smoother surfaces, but causing more blurring in the output optical map. 
              <br>
            </div>

            <button class="collapsible" id="collapsible-button-32" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><strong><br>CNN</strong><br></button>
            <div class="collapsible-content" id="collapsible-section-32">            
              <br>In Update 1, we trained the CNN on frames with minimal preprocessing. The results showed a validation set MSE of 53.52. In this update we improved upon our results for the CNN by trying different methods.<br><br>

              Our overall CNN setup followed the architecture outlined in the NVIDIA paper,but we added a dropout layer to reduce overfitting to training data. We also tried many different optimizers, but the Adam optimizer provided the best results consistently. Our training/testing split for the data was 75/25, rather than 80/20 that was used last time in the midterm update. This was also to reduce overfitting the training data. We also tried different preprocessing approaches to further improve results.

              <br><br>
              The first approach used in this update was to use Canny thresholding to emphasize edges of objects and then use an Optical Flow algorithm. With this method we got a validation set MSE of 40.634, which is a little better than in the first update. We noticed that the training data had an MSE of around 0.2891, so we thought the model was overfitting the training data. To combat the overfitting, we added a Dropout Layer to the CNN. After running the same preprocessing on the new CNN architecture, we got a validation set MSE of 35. 516 and a training set MSE of 1.149. The next approach was to run the optical flow algorithm on just the raw black and white frames instead of the results from Canny Thresholding. This resulted in a validation set MSE of 27.156 and a training set MSE of 0.6896. Each iteration offered slightly better results.<br><br>

              <table>
                <colgroup>
                  <col span="1" style="width: 60%;">
                  <col span="1" style="width: 20%;">
                  <col span="1" style="width: 20%;">
               </colgroup>
                <tbody>
                  <tr>
                    <th>Preprocessing</th>
                    <th>Training MSE</th>
                    <th>Validation MSE</th>
                  </tr>
                  <tr>
                    <td>Cropped + Resized Image Only</td>
                    <td>25.86</td>
                    <td>53.52</td>
                  </tr>
                  <tr>
                    <td>Optical Flow + Canny</td>
                    <td>0.29</td>
                    <td>40.64</td>
                  </tr>
                  <tr>
                    <td>Optical Flow + Dropout Layer</td>
                    <td>0.69</td>
                    <td>27.16</td>
                  </tr>
                  <tr>
                    <td>Optical Flow + Canny + Dropout Layer</td>
                    <td>1.15</td>
                    <td>35.52</td>
                  </tr>
                </tbody>
              </table>
              Thus we found the best approach to minimize the validation MSE was to use Optical Flow and to drop a layer from the NVIDIA CNN, to achieve an MSE of 27.16.
              
            </div>
          </p>
          <hr>
        </div>

        <button class="collapsible" id="collapsible-button-4" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><h3>Qualitative Results</h3></button>
        <div class="collapsible-content" id="collapsible-section-4">
          <p>
            <button class="collapsible" id="collapsible-button-40" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><strong>SIFT, Lane Detection, Homography</strong></button>
            <div class="collapsible-content" id="collapsible-section-40">            
              <div align="center"><u>Bad Case of SIFT Implementation </u></div>
              <figure align="center">
                <!-- <a target="_blank" href="https://www.youtube.com/watch?v=mbl6ttHtSFI&feature=youtu.be">
                  <img src="media/siftbad.gif" style="width:100%">
                </a> -->
                <iframe class="center" width="420" height="345" src="https://www.youtube.com/embed/mbl6ttHtSFI">
                </iframe>
                <figcaption>Blue (Top-Left): True Speeds <br> Green (Top-Right): Predicted Speeds</figcaption>
              </figure><br>

              <div align="center"><u>Better Case of SIFT Implementation </u></div>
              <figure align="center">
                <!-- <a target="_blank" href="https://www.youtube.com/watch?v=fF56I4QNeX4&feature=youtu.be">
                  <img src="media/siftgood.gif" style="width:100%">
                </a> -->
                <iframe class="center" width="420" height="345" src="https://www.youtube.com/embed/fF56I4QNeX4">
                </iframe>
                <figcaption>Blue (Top-Left): True Speeds <br> Green (Top-Right): Predicted Speeds</figcaption>
              </figure><br>

              With the SIFT implementation, the approach works quite well once the homography from the previous frame to the next is determined accurately. However, the biggest issue we ran into was the lack of SIFT keypoints on the road. Since the road is mostly plain black/gray with a little marking for lanes, the keypoints detected by SIFT were mainly on other vehicles on the road. This interferes with our ability to calculate accurate homography since we want the speed relative to the road, not other vehicles.<br><br>

              We tried filtering out keypoints to road markings by applying lane detection, but this yielded none or too few keypoints in many frames necessary to calculate the homography. If there still is not enough, a simple road region polygon mask is applied to the image to include more keypoints.<br><br>

              This approach did not yield the best result but appears possible with refinements.

              <br><br>
            </div>

            <button class="collapsible" id="collapsible-button-41" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><strong>Dense Optical Map and CNN</strong></button>
            <div class="collapsible-content" id="collapsible-section-41">            
              <br>
              <div align="center"><u>Preprocessing for CNN</u></div>
              Below is an example of a cropped and resized image without further preprocessing, vs. optical flow preprocessing.

              <figure align="center">
                <img src="media/oflow.png" style="width:100%">
              </figure><br>

              <div align="center"><u>MSE for Training vs. Validation</u></div>
              Here are how our MSE's improved each epoch for the training data set, and for the validation data set, using our CNN with Canny + Optical Flow Preprocessed inputs. Note that our values of 40-50 match those in our table for Canny + Optical (40.64).

              <figure align="center">
                <img src="media/mse.png" style="width:100%">
              </figure>
              <br>
            </div>
          </p>
          <hr>
        </div>

        <button class="collapsible" id="collapsible-button-5" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><h3>Conclusion and Future Work</h3></button>
        <div class="collapsible-content" id="collapsible-section-5">
          <p>
            We were able to improve our MSE by a decrement of ~10 to ~25 after the midterm update, but there is still a lot of areas for improvement. <br><br>

            After testing out various training models (CNN, MLP regressor, etc), we felt the pre-processing phase is much more important for better results, though the CNN had comparatively better results than other models when keeping the input data the same. We think, at a high level, one of the major issues is <b>lane detection</b>, which can help improve tracking the car’s relative speed frame to frame. A few of the prior implementations of the challenge which got MSE to be less than 10, had really good lane detection. The speed of each frame, calculated via feeding the optical flow into the neural network model, performs better, the better the lanes are identifiable and highlighted in the grayscaled image. <br><br>
            One way we could try and improve on this is by adding <b>Semantic Segmentation</b>, which can be used to get rid of static objects and surrounding cars in the camera view, alongside the Canny Edge detector we set up in the dense optical approach, and with the Hough Line Transformation/homography via the SIFT approach, except this time robustly combine both pre-processing techniques.
            <br><br>
            Additionally, there were some other improvements we wanted to explore as stated in the Midterm Update for the Final Update, that we didn’t have the chance to do, but we think would still be useful to see how it would affect the MSE. Some of them were: <br>
            <ul>
              <li>Trying out other variants of CNN models. Right now, we use NVIDIA CNN as diagrammed above. Although we believe the results didn’t fluctuate dramatically, given more time, it will help to see how the MSE stacks against other CNN structures/models.</li>
              <li>Cropping more effectively - we minimized the frame rectangularly when really it should be a trapezoidal-esque crop.</li>
            </ul>
            
          </p>
          <hr>
        </div>

        <button class="collapsible" id="collapsible-button-6" onclick="toggleCollapsibleSectionWithAnimation.call(this)"><h3>References</h3></button>
        <div class="collapsible-content" id="collapsible-section-6">
          <p>
            <ol>
              <li>
                End to End Learning for Self-Driving Cars, NVIDIA Corporation. <br> <a target="_blank" href="https://arxiv.org/pdf/1604.07316v1.pdf">https://arxiv.org/pdf/1604.07316v1.pdf</a>
              </li>
            </ol>
          </p>
          <hr>
        </div>

      </section>
    </div>
    <script>
        var metas = document.getElementsByTagName('meta');
        var i;
        if (navigator.userAgent.match(/iPhone/i)) {
          for (i=0; i<metas.length; i++) {
            if (metas[i].name == "viewport") {
              metas[i].content = "width=device-width, minimum-scale=1.0, maximum-scale=1.0";
            }
          }
          document.addEventListener("gesturestart", gestureStart, false);
        }
        function gestureStart() {
          for (i=0; i<metas.length; i++) {
            if (metas[i].name == "viewport") {
              metas[i].content = "width=device-width, minimum-scale=0.25, maximum-scale=1.6";
            }
          }
        }
    
        function toggleCollapsibleSectionWithAnimation() {
          this.classList.toggle("collapsible-active");
          var buttonId = this.id;
          var sectionId = buttonId.replace("button","section");
          var content = document.getElementById(sectionId);
          var mHeight = window.getComputedStyle(content).maxHeight;
          if (mHeight !== "0px"){
            content.style.maxHeight = "0px";
          } else {
            content.style.maxHeight = "100%";
          }
        }
    </script>
    
  </body>
</html>
